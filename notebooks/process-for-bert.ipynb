{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduard/dep-tasks/prob-topic-model/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "from tqdm import notebook as tqdm\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "\n",
    "SEP_TOKEN = '<sep>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2, palette='Set2')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.markersize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 24\n",
    "plt.rcParams['legend.fontsize'] = 24\n",
    "plt.rcParams['axes.titlesize'] = 30\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 7)\n",
    "\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на распределение секций внутри текста.\n",
    "\n",
    "Секция -- всё, что обёрнуто в `==`\n",
    "\n",
    "Из примеров выше: `== Майндмап ==`, `== Аннотация ==`, `== Видео ==`\n",
    "\n",
    "Далее фильтруем по числу документов в секции: их должно быть не менее 20.\n",
    "\n",
    "В конце сортируем по числу документов в секции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0x1tv-dataset.pickle', 'rb') as fd:\n",
    "    data = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем 3 домена: заголовок, аннотация и тезисы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_position(*positions):\n",
    "    return min([pos for pos in positions if pos >= 0], default=len(data))\n",
    "\n",
    "def extract_section_text(data, section_name):\n",
    "    if section_name == 'Аннотация':\n",
    "        blockquote_pattern = r'<blockquote>(.*?)</blockquote>'\n",
    "        match = re.search(blockquote_pattern, data, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "    \n",
    "    section_pattern = fr'==\\s*{re.escape(section_name)}\\s*=='\n",
    "    section_start = re.search(section_pattern, data)\n",
    "    \n",
    "    if not section_start:\n",
    "        return None\n",
    "   \n",
    "    position = section_start.end()\n",
    "\n",
    "    # Look for stop tokens and the start of next sections\n",
    "    stop_tokens = ['{{----}}', '{{LinksSection}}', '== Примечания и отзывы ==']\n",
    "    ends = [data.find(token, position) for token in stop_tokens]\n",
    "    end_position = get_min_position(*ends)\n",
    "    \n",
    "    if end_position != len(data):\n",
    "        return data[position:end_position].strip()\n",
    "    \n",
    "    return data[position:].strip()\n",
    "\n",
    "def extract_sections(data):\n",
    "    sections = ['Аннотация', 'Thesis', 'Тезисы', 'Расширенные тезисы']\n",
    "    extracted_texts = {section: extract_section_text(data, section) for section in sections}\n",
    "    return extracted_texts\n",
    "\n",
    "\n",
    "def coalesce(dct, keys):\n",
    "    for key in keys:\n",
    "        if dct[key] is not None or len(dct[key]) > 0:\n",
    "            return dct[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_document(doc):\n",
    "    parsed_doc = dict()\n",
    "    parsed_doc['title'] = doc['title']\n",
    "    \n",
    "    sections_data = extract_sections(doc['text'])\n",
    "    parsed_doc['annotation'] = sections_data['Аннотация']\n",
    "    parsed_doc['thesis'] = coalesce(sections_data, ['Thesis', 'Тезисы', 'Расширенные тезисы'])\n",
    "    \n",
    "    return parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим ФИО докладчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число докладов без спикеров: 74\n",
      "Число докладов с одним спикером: 2313\n",
      "Число докладов с двумя спикерами: 119\n",
      "Число докладов с не менее тремя спикерами: 35\n"
     ]
    }
   ],
   "source": [
    "def speaker_detect(text):\n",
    "    speakers = re.findall('(?<={{Speaker\\|).*(?=}})', text)\n",
    "    preproc_speakers = []\n",
    "    for speaker in speakers:\n",
    "        preproc_speakers.extend(speaker.split('|'))\n",
    "    return preproc_speakers\n",
    "\n",
    "\n",
    "all_speakers = [speaker_detect(text['text']) for text in data['articles']]\n",
    "\n",
    "print('Число докладов без спикеров:', len(list(filter(lambda x: len(x) == 0, all_speakers))))\n",
    "print('Число докладов с одним спикером:', len(list(filter(lambda x: len(x) == 1, all_speakers))))\n",
    "print('Число докладов с двумя спикерами:', len(list(filter(lambda x: len(x) == 2, all_speakers))))\n",
    "print('Число докладов с не менее тремя спикерами:', len(list(filter(lambda x: len(x) >= 3, all_speakers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтруем категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_categories(sample):\n",
    "    return [re.sub('Категория:', '', el) for el in sample]\n",
    "\n",
    "\n",
    "text_categories = [clear_categories(item['categories']) for item in data['articles']]\n",
    "child_categories = [item['title'] for item in data['categories']]\n",
    "parent_categories = [clear_categories(item['categories']) for item in data['categories']]\n",
    "\n",
    "ALL_CATEGORIES = list(set(itertools.chain(*text_categories)) | set(child_categories) | set(itertools.chain(*parent_categories)))\n",
    "\n",
    "child_to_parent_categories = {item['title']: clear_categories(item['categories']) for item in data['categories']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим вручную все имена из списка категорий + `HasSpeaker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CATEGORIES_WITHOUT_NAMES = set([\n",
    "    '.NET',\n",
    "    '1C',\n",
    "    'ALT Linux',\n",
    "    'ALTLinux на Эльбрусе',\n",
    "    'AR',\n",
    "    'AWS',\n",
    "    'Accessibility',\n",
    "    'Agile',\n",
    "    'Agile Introduction',\n",
    "    'Agile process',\n",
    "    'Agile в корпорациях',\n",
    "    'Agile — технологические практики',\n",
    "    'Agile&Lean Mindset',\n",
    "    'Agile-культура',\n",
    "    'Agile-масштабирование',\n",
    "    'Agile-преобразования',\n",
    "    'Alfresco',\n",
    "    'Ansible',\n",
    "    'Arduino',\n",
    "    'AstraLinux',\n",
    "    'Atlassian',\n",
    "    'Azure',\n",
    "    'B2B продукты',\n",
    "    'BDD',\n",
    "    'BigData',\n",
    "    'Blockchain',\n",
    "    'Bluemix',\n",
    "    'Business rules engine',\n",
    "    'C++',\n",
    "    'CMMI',\n",
    "    'CQRS',\n",
    "    'CRIU',\n",
    "    'Clouds',\n",
    "    'Clsync',\n",
    "    'Code Review',\n",
    "    'Collaboration tools',\n",
    "    'Configuration Management',\n",
    "    'ContactOK',\n",
    "    'Continuous Integration',\n",
    "    'CouchDB',\n",
    "    'Csharp',\n",
    "    'CustisWikiToLib',\n",
    "    'Customer Journey Map',\n",
    "    'DDD',\n",
    "    'DSL-языки',\n",
    "    'Data Analysis',\n",
    "    'Deployment',\n",
    "    'Design Thinking',\n",
    "    'DevOps',\n",
    "    'Draft',\n",
    "    'E-commerce',\n",
    "    'Embox',\n",
    "    'Erlang',\n",
    "    'Extreme Programming',\n",
    "    'Feature Branches',\n",
    "    'Firefox',\n",
    "    'Foresight management',\n",
    "    'FreeIPA',\n",
    "    'Front end development',\n",
    "    'Fsharp',\n",
    "    'GWT',\n",
    "    'Glibc',\n",
    "    'Go',\n",
    "    'Groovy',\n",
    "    'Growth Hacking',\n",
    "    'HR',\n",
    "    'Hardware',\n",
    "    'Health',\n",
    "    'High Performace Computing',\n",
    "    'Highload-архитектуры',\n",
    "    'IP-телефония',\n",
    "    'IT-законы',\n",
    "    'IT-образование',\n",
    "    'Impact Map',\n",
    "    'Information Security',\n",
    "    'Internet of Thing',\n",
    "    'Java',\n",
    "    'Java EE',\n",
    "    'Javascript',\n",
    "    'Jenkins',\n",
    "    'Kanban',\n",
    "    'Knowledge Management',\n",
    "    'Kotlin',\n",
    "    'Kubernetes',\n",
    "    'LAMP',\n",
    "    'LSM',\n",
    "    'LeSS',\n",
    "    'Lean',\n",
    "    'Lean Startup',\n",
    "    'Legacy',\n",
    "    'Libreoffice',\n",
    "    'Linux',\n",
    "    'Linux для Эльбруса',\n",
    "    'Linux-дистрибутивы',\n",
    "    'Linux-дистрибутивы для Enterprise',\n",
    "    'Linux-обучение',\n",
    "    'Lua',\n",
    "    'MIPS',\n",
    "    'Machine Learning',\n",
    "    'Mechanics',\n",
    "    'Microsoft',\n",
    "    'Misc',\n",
    "    'Mkimage-profiles',\n",
    "    'MongoDB',\n",
    "    'Morpheus',\n",
    "    'MySQL',\n",
    "    'NLP',\n",
    "    'NOSQL',\n",
    "    'NUI',\n",
    "    'Natural Language Processing',\n",
    "    'NeedContacts',\n",
    "    'Nemerle',\n",
    "    'Node.js',\n",
    "    'Object Oriented Programming',\n",
    "    'Open-source',\n",
    "    'Open-source CAD',\n",
    "    'Open-source CMS',\n",
    "    'Open-source CRM',\n",
    "    'Open-source ERP',\n",
    "    'Open-source PAAS',\n",
    "    'Open-source TCO',\n",
    "    'Open-source and Community',\n",
    "    'Open-source and hardware',\n",
    "    'Open-source communications',\n",
    "    'Open-source for Enterprise',\n",
    "    'Open-source operating systems',\n",
    "    'Open-source projects',\n",
    "    'Open-source СУБД',\n",
    "    'Open-source и законы',\n",
    "    'OpenShift',\n",
    "    'OpenStack',\n",
    "    'OpenVZ',\n",
    "    'PAAS',\n",
    "    'PHP',\n",
    "    'Pascale Xelot-Dugat',\n",
    "    'People Management',\n",
    "    'Pivot',\n",
    "    'PostgreSQL',\n",
    "    'ProductMeetup',\n",
    "    'Python',\n",
    "    'RISC-V',\n",
    "    'ROSA Linux',\n",
    "    'ROSALab',\n",
    "    'RTOS',\n",
    "    'Reviewed',\n",
    "    'Riak',\n",
    "    'Ruby',\n",
    "    'RunaWFE',\n",
    "    'SAP',\n",
    "    'SELinux',\n",
    "    'SOLID',\n",
    "    'SVM',\n",
    "    'Samba',\n",
    "    'Scala',\n",
    "    'Scrum',\n",
    "    'Serverless',\n",
    "    'Sharepoint',\n",
    "    'SkillsWiki',\n",
    "    'Skype',\n",
    "    'Software Defined Networks',\n",
    "    'Strace',\n",
    "    'Support',\n",
    "    'TAU-платформа',\n",
    "    'TDD',\n",
    "    'Talks in English',\n",
    "    'Tarantool',\n",
    "    'Taucraft',\n",
    "    'Team Communication',\n",
    "    'Teмы',\n",
    "    'ToPublish',\n",
    "    'Tuukka Ahoniemi',\n",
    "    'UI',\n",
    "    'UI SmartTV',\n",
    "    'UI бизнес-приложений',\n",
    "    'UX',\n",
    "    'UX + Agile',\n",
    "    'UX проектирование',\n",
    "    'User Story',\n",
    "    'VCS',\n",
    "    'Visual Studio',\n",
    "    'Waterfall',\n",
    "    'WebRTC',\n",
    "    'Windows',\n",
    "    'World Usability Day',\n",
    "    'ZFS',\n",
    "    'Zabbix',\n",
    "    'Автоматизированное тестирование',\n",
    "    'Алгоритмы',\n",
    "    'Анализ программ и систем',\n",
    "    'Аналитика',\n",
    "    'Архитектура',\n",
    "    'Архитектура информационных систем',\n",
    "    'Архитектура серверных приложений',\n",
    "    'Аутентификация и авторизация',\n",
    "    'БПЛА',\n",
    "    'Базы данных',\n",
    "    'Байкал',\n",
    "    'Безопасность',\n",
    "    'Бизнес в IT',\n",
    "    'Бизнес и СПО',\n",
    "    'Бизнес-анализ',\n",
    "    'Блиц-доклады',\n",
    "    'Веб-дизайн',\n",
    "    'Веб-разработка',\n",
    "    'Вебинары PingWin',\n",
    "    'Верификация',\n",
    "    'Видеосвязь',\n",
    "    'Визуализация',\n",
    "    'Виртуализация',\n",
    "    'Виртуальный ассистент',\n",
    "    'Встраиваемые системы',\n",
    "    'Выход на зарубежные рынки',\n",
    "    'Геймификация',\n",
    "    'Геймификация в UX',\n",
    "    'Геолокация',\n",
    "    'Голосовой интерфейс',\n",
    "    'Госсектор',\n",
    "    'Государство и софт',\n",
    "    'Графовые базы данных',\n",
    "    'Диаграммы Кано',\n",
    "    'Дизайн',\n",
    "    'Динамический анализ',\n",
    "    'Дискуссии',\n",
    "    'Дискуссии о юзабилити',\n",
    "    'Доверенная загрузка',\n",
    "    'Доклад со стенограммой',\n",
    "    'Докладчики',\n",
    "    'Доклады на английском',\n",
    "    'Доклады на белорусском языке',\n",
    "    'Доклады на иностранных языках',\n",
    "    'Доклады на украинском',\n",
    "    'Документация и Agile',\n",
    "    'Документирование',\n",
    "    'Запуск продукта',\n",
    "    'Запуск продукта в Retail',\n",
    "    'Инструменты майнтейнера',\n",
    "    'Инструменты майнтейнеров',\n",
    "    'Инструменты разработки',\n",
    "    'Информационная безопасность',\n",
    "    'Информационная безопасность и СПО',\n",
    "    'Информационные системы ВУЗов',\n",
    "    'Использование open-source',\n",
    "    'Исследовательское тестирование',\n",
    "    'История из практики',\n",
    "    'Картография',\n",
    "    'Карьера в IT',\n",
    "    'Командообразование',\n",
    "    'Компиляторы',\n",
    "    'Компиляция под Linux',\n",
    "    'Компьютерная графика',\n",
    "    'Компьютерное зрение',\n",
    "    'Конференции',\n",
    "    'Корпоративные решения',\n",
    "    'Криптография',\n",
    "    'Кроссплатформенная разработка',\n",
    "    'Круглый стол',\n",
    "    'Кумир',\n",
    "    'Лидерство',\n",
    "    'Линейки конференций',\n",
    "    'Локализация',\n",
    "    'Маркетинг',\n",
    "    'Мастер-классы',\n",
    "    'Менеджмент',\n",
    "    'Метрики качества',\n",
    "    'Микропрограммирование',\n",
    "    'Микросервисы',\n",
    "    'Мобильная разработка',\n",
    "    'Моделирование бизнес-процессов',\n",
    "    'Моделирование физических систем',\n",
    "    'Монетизация',\n",
    "    'Мониторинг',\n",
    "    'Мотивация',\n",
    "    'Наука',\n",
    "    'Облачные сервисы',\n",
    "    'Образование',\n",
    "    'Обучение',\n",
    "    'Обучение бизнес-анализу',\n",
    "    'Обучение бизнес-процессам',\n",
    "    'Обучение проектному менеджменту',\n",
    "    'Обучение системному программированию',\n",
    "    'Онлайн-обучение',\n",
    "    'Операционные системы',\n",
    "    'Оптимизация приложения',\n",
    "    'Опыт внедрения СПО',\n",
    "    'Организационные изменения',\n",
    "    'Организационный анализ',\n",
    "    'Открытые данные',\n",
    "    'Отладка',\n",
    "    'Оценка сотрудников',\n",
    "    'Очереди',\n",
    "    'Параллельное программирование',\n",
    "    'Пиктомир',\n",
    "    'Планирование',\n",
    "    'Планирование в Agile',\n",
    "    'Планировка задач',\n",
    "    'Привественные речи',\n",
    "    'Программирование',\n",
    "    'Программная архитектура',\n",
    "    'Продуктовая аналитика',\n",
    "    'Прототипирование UI',\n",
    "    'Процесс разработки',\n",
    "    'Процесс разработки UX и UI',\n",
    "    'Процесс тестирования',\n",
    "    'Психология пользователя',\n",
    "    'Психология разработки',\n",
    "    'Разработка open-source',\n",
    "    'Разработка десктоп-приложений',\n",
    "    'Разработка десктопных приложений под Windows',\n",
    "    'Разработка игр',\n",
    "    'Разработка операционных систем',\n",
    "    'Распределенные системы',\n",
    "    'Редкие языки программирования',\n",
    "    'Реклама',\n",
    "    'Реклама компании',\n",
    "    'Рекомендательные системы',\n",
    "    'Ретроспектива',\n",
    "    'Рефакторинг',\n",
    "    'Робототехника',\n",
    "    'С++',\n",
    "    'САПР',\n",
    "    'СПО в Госуправлении',\n",
    "    'СПО в России',\n",
    "    'СПО в науке',\n",
    "    'СПО в образовании',\n",
    "    'СПО в творчестве',\n",
    "    'СПО для системного администрирования',\n",
    "    'СУБД',\n",
    "    'Свободные библиотеки построения графиков',\n",
    "    'Свободные лицензии',\n",
    "    'Сервис-ориентированная архитектура',\n",
    "    'Сети',\n",
    "    'Системное администрирование',\n",
    "    'Системное мышление',\n",
    "    'Системный анализ',\n",
    "    'Системный подход',\n",
    "    'Системы управления версиями',\n",
    "    'Скрытые категории',\n",
    "    'Собрания ALT.NET',\n",
    "    'Совещания',\n",
    "    'Стажировка',\n",
    "    'Статический анализ кода',\n",
    "    'Стратегическое планирование',\n",
    "    'ТРИЗ',\n",
    "    'Темы',\n",
    "    'Теория ограничений',\n",
    "    'Тестирование',\n",
    "    'Тестирование UI',\n",
    "    'Тестирование игр',\n",
    "    'Тестирование мобильных приложений',\n",
    "    'Тестирование производительности',\n",
    "    'Технологии',\n",
    "    'Технологии будущего',\n",
    "    'Технологии крупных вендоров',\n",
    "    'Тренды Open-source',\n",
    "    'Умные вещи',\n",
    "    'Управление заинтересованными сторонами',\n",
    "    'Управление качеством',\n",
    "    'Управление продуктами',\n",
    "    'Управление рисками',\n",
    "    'Управление собой',\n",
    "    'Управление техподдержкой',\n",
    "    'Управление требованиями',\n",
    "    'Файловые системы',\n",
    "    'Философия программирования',\n",
    "    'Философия юзабилити',\n",
    "    'Финансовые системы',\n",
    "    'Фреймворки',\n",
    "    'Фриланс',\n",
    "    'Функциональное программирование',\n",
    "    'Хранение данных',\n",
    "    'Хэши',\n",
    "    'Эксплуатация',\n",
    "    'Эльбрус',\n",
    "    'Юзабилити',\n",
    "    'Юзабилити в 1C',\n",
    "    'Юзабилити в играх',\n",
    "    'Юзабилити интернет-магазинов',\n",
    "    'Юзабилити исследования',\n",
    "    'Юзабилити мобильных устройств',\n",
    "    'Юзабилити образование',\n",
    "    'Юзабилити поиска',\n",
    "    'Юзабилити текста',\n",
    "    'Языки программирования',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce(dct, keys):\n",
    "    for key in keys:\n",
    "        if dct[key] is not None:\n",
    "            return dct[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_document_with_categories(doc):\n",
    "    parsed_doc = dict()\n",
    "    parsed_doc[\"title\"] = doc[\"title\"]\n",
    "    parsed_doc['speakers'] = speaker_detect(doc['text'])\n",
    "    \n",
    "    sections_data = extract_sections(doc['text'])\n",
    "    parsed_doc[\"annotation\"] = sections_data[\"Аннотация\"]\n",
    "    parsed_doc[\"thesis\"] = coalesce(\n",
    "        sections_data, [\"Thesis\", \"Тезисы\", \"Расширенные тезисы\"]\n",
    "    )\n",
    "    \n",
    "    parsed_doc['raw_categories'] = clear_categories(doc[\"categories\"])\n",
    "    parsed_doc[\"categories\"] = list(\n",
    "        filter(\n",
    "            lambda x: x in ALL_CATEGORIES_WITHOUT_NAMES,\n",
    "            parsed_doc['raw_categories'],\n",
    "        )\n",
    "    )\n",
    "    return parsed_doc\n",
    "\n",
    "\n",
    "data_raw = [parse_document_with_categories(doc) for doc in data['articles']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как большие языковые модели довольно \"умные\", то можно спокойно опустить большую часть фильтрации текста.\n",
    "\n",
    "Причём, если эту фильтрацию оставить, то полученные эмбеддинги окажутся не такими хорошими, поскольку обработанный текст менее логичен и связен, в отличии от естественного языка.\n",
    "\n",
    "Тем не менее, мы оставим некоторые фильтры: это удаление ссылок и сохранение токена <sep> между разными модальностями\n",
    "\n",
    "Очистка текста:\n",
    "- удаление ссылок\n",
    "- удаление всех HTML тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пример текста. Это тестовый текст с примером ссылки.\n",
      "Вот еще ссылка: \n",
      "Здесь есть часто-встречающиеся слова, например, \"тест\" много раз.\n",
      "...\n",
      "Также нужно удалить .эти строки.\n",
      "\n",
      "{'text': ['Маркетинг', 'мобильных', 'приложений', '(Юрий', 'Мельничек,', 'SECR-2015)', '<sep>', 'Я', 'руковожу', 'MAPS.ME', 'с', 'самого', 'начала', 'и', 'до', '25', 'миллионов', 'инсталляций', 'на', 'текущий', 'момент.', 'За', 'это', 'время', 'у', 'меня', 'сформировался', 'системный', 'взгляд', 'на', 'маркетинг', 'мобильных', 'приложений,', 'которым', 'я', 'и', 'поделюсь', 'со', 'слушателями:', '*', 'Какие', 'существуют', 'маркетинговые', 'каналы', 'для', 'раскрутки', 'мобильных', 'приложений', '(PR,', 'ASO,', 'реклама,', 'фичеринги', 'и', 'т.д).', 'Как', 'работать', 'с', 'каждым', 'из', 'них.', '*', 'Какой', 'путь', 'проходят', 'пользователи', 'до', 'инсталляции', 'приложения.', '*', 'Как', 'увеличить', 'конверсию.', '*', 'Какие', 'есть', 'инструменты', 'мобильного', 'маркетинга.', '*', 'Какое', 'положение', 'маркетинга', 'в', 'бизнес-стратегии', 'мобильного', 'приложения.', 'Доклад', 'будет', 'интересен', 'маркетологам,', 'топ-менеджерам', 'и', 'разработчикам', 'мобильных', 'приложений.'], 'categories': ['Мобильная разработка', 'Управление продуктами'], 'raw_categories': ['HasSpeaker', 'SECR-2015', 'Мобильная разработка', 'Управление продуктами', 'Юрий Мельничек'], 'speakers': ['Юрий Мельничек']}\n"
     ]
    }
   ],
   "source": [
    "def clean_text_for_llm(text):\n",
    "    if text is None or len(text) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_and_merge_document_for_llm(doc, sep_token=SEP_TOKEN):\n",
    "    cleaned_texts = []\n",
    "\n",
    "    # Clean each field and collect non-empty results\n",
    "    for field in [\"title\", \"annotation\", \"thesis\"]:\n",
    "        field_text = doc.get(field, \"\")\n",
    "        cleaned_tokens = clean_text_for_llm(field_text)\n",
    "\n",
    "        if cleaned_tokens:\n",
    "            cleaned_texts.append(cleaned_tokens)\n",
    "\n",
    "    # Join with <sep> token\n",
    "    result_text = f\" {sep_token} \".join(cleaned_texts).strip()\n",
    "\n",
    "    result = {\n",
    "        \"text\": result_text.split(),\n",
    "        \"categories\": doc.get(\"categories\", []),\n",
    "        \"raw_categories\": doc.get(\"raw_categories\", []),\n",
    "        \"speakers\": doc.get(\"speakers\", []),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "<html>Пример текста. Это тестовый текст с примером <a href=\"http://example.com\">ссылки</a>.\n",
    "Вот еще ссылка: https://www.test.ru.\n",
    "Здесь есть часто-встречающиеся слова, например, \"тест\" много раз.\n",
    "...\n",
    "Также нужно удалить .эти строки.\n",
    "\"\"\"\n",
    "\n",
    "print(clean_text_for_llm(text))\n",
    "print(clean_and_merge_document_for_llm(data_raw[1298]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [clean_and_merge_document_for_llm(doc) for doc in data_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text with mBERT -- multi-lingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(texts, model, tokenizer):\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    )\n",
    "    device = model.device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"].to(device),\n",
    "            attention_mask=encoding[\"attention_mask\"].to(device)\n",
    "        )\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state.mean(dim=(0, 1))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DOWNLOADED = True\n",
    "\n",
    "if not IS_DOWNLOADED:\n",
    "    model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "    model.save_pretrained('models/m-bert')\n",
    "    tokenizer.save_pretrained('tokenizers/m-bert')\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(\"models/m-bert\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tokenizers/m-bert\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_v2 = [\n",
    "    {\n",
    "        'text': encode_text(doc['text'], model, tokenizer).cpu().numpy(),\n",
    "        'categories': doc['categories'],\n",
    "        'raw_categories': doc['raw_categories'],\n",
    "        'speakers': doc['speakers']\n",
    "    } for doc in processed_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = {ctgry: encode_text(ctgry, model, tokenizer).cpu().numpy() for ctgry in ALL_CATEGORIES_WITHOUT_NAMES} \n",
    "\n",
    "with open('data/label_embeddings.pkl', 'wb') as fd:\n",
    "    pickle.dump(label_embeddings, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(processed_data_v2)\n",
    "\n",
    "unravelled_values = df['text'].explode().values.reshape(len(df), -1)\n",
    "unravelled_df = pd.DataFrame(unravelled_values, dtype=float)\n",
    "unravelled_df.columns = list(map(str, unravelled_df.columns))\n",
    "\n",
    "df_final = pd.concat((df.iloc[:, 1:], unravelled_df), axis=1)\n",
    "df_final.to_parquet('data/m_bert_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
